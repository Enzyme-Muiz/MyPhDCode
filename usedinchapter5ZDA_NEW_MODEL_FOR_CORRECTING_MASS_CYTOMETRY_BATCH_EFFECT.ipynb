{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##differentiate MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path\n",
    "#!pip install protobuf==3.20.*\n",
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constraints\n",
    "def positivity(f):\n",
    "    '''\n",
    "    Constraint 1: \n",
    "    Ensures flow moves from source to target\n",
    "    '''\n",
    "    return f \n",
    "\n",
    "def fromSrc(f, wp, i, shape):\n",
    "    \"\"\"\n",
    "    Constraint 2: \n",
    "    Limits supply for source according to weight\n",
    "    \"\"\"\n",
    "    fr = np.reshape(f, shape)\n",
    "    f_sumColi = np.sum(fr[i,:])\n",
    "    return wp[i] - f_sumColi\n",
    "\n",
    "def toTgt(f, wq, j, shape):\n",
    "    \"\"\"\n",
    "    Constraint 3: \n",
    "    Limits demand for target according to weight\n",
    "    \"\"\"\n",
    "    fr = np.reshape(f, shape)\n",
    "    f_sumRowj = np.sum(fr[:,j])\n",
    "    return wq[j] - f_sumRowj\n",
    "\n",
    "def maximiseTotalFlow(f, wp, wq): \n",
    "    \"\"\"\n",
    "    Constraint 4: \n",
    "    Forces maximum supply to move from source to target\n",
    "    \"\"\"\n",
    "    return f.sum() - np.minimum(wp.sum(), wq.sum())\n",
    "\n",
    "# Objective function\n",
    "def flow(f, D):\n",
    "    \"\"\"\n",
    "    The objective function\n",
    "    The flow represents the amount of goods to be moved \n",
    "    from source to target\n",
    "    \"\"\"\n",
    "    f = np.reshape(f, D.shape)\n",
    "    return (f * D).sum()\n",
    "\n",
    "# Distance\n",
    "def groundDistance(x1, x2, norm = 2):\n",
    "    \"\"\"\n",
    "    L-norm distance\n",
    "    Default norm = 2\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x1-x2, norm)\n",
    "\n",
    "# Distance matrix\n",
    "def getDistMatrix(s1, s2, norm = 2):\n",
    "    \"\"\"\n",
    "    Computes the distance matrix between the source\n",
    "    and target distributions.\n",
    "    The ground distance is using the L-norm (default L2 norm)\n",
    "    \"\"\"\n",
    "    # Slow method\n",
    "    # rows = s1 feature length\n",
    "    # cols = s2 feature length\n",
    "    numFeats1 = s1.shape[0]\n",
    "    numFeats2 = s2.shape[0]\n",
    "    distMatrix = np.zeros((numFeats1, numFeats2))\n",
    "\n",
    "    for i in range(0, numFeats1):\n",
    "        for j in range(0, numFeats2):\n",
    "            distMatrix[i,j] = groundDistance(s1[i], s2[j], norm)\n",
    "\n",
    "    # Fast method (requires scipy.spatial)\n",
    "    #import scipy.spatial\n",
    "    #distMatrix = scipy.spatial.distance.cdist(s1, s2)\n",
    "\n",
    "    return distMatrix\n",
    "\n",
    "# Flow matrix\n",
    "def getFlowMatrix(P, Q, D):\n",
    "    \"\"\"\n",
    "    Computes the flow matrix between P and Q\n",
    "    \"\"\"\n",
    "    numFeats1 = P[0].shape[0]\n",
    "    numFeats2 = Q[0].shape[0]\n",
    "    shape = (numFeats1, numFeats2)\n",
    "\n",
    "    # Constraints  \n",
    "    cons1 = [{'type':'ineq', 'fun' : positivity},\n",
    "             {'type':'eq', 'fun' : maximiseTotalFlow, 'args': (P[1], Q[1],)}]\n",
    "\n",
    "    cons2 = [{'type':'ineq', 'fun' : fromSrc, 'args': (P[1], i, shape,)} for i in range(numFeats1)]\n",
    "    cons3 = [{'type':'ineq', 'fun' : toTgt, 'args': (Q[1], j, shape,)} for j in range(numFeats2)]\n",
    "\n",
    "    cons = cons1 + cons2 + cons3\n",
    "\n",
    "    # Solve for F (solve transportation problem) \n",
    "    F_guess = np.zeros(D.shape)\n",
    "    F = scipy.optimize.minimize(flow, F_guess, args=(D,), constraints=cons)\n",
    "    F = np.reshape(F.x, (numFeats1,numFeats2))\n",
    "\n",
    "    return F\n",
    "\n",
    "# Normalised EMD\n",
    "def EMD(F, D):  \n",
    "    \"\"\"\n",
    "    EMD formula, normalised by the flow\n",
    "    \"\"\"\n",
    "    return (F * D).sum() / F.sum()\n",
    "\n",
    "# Runs EMD program  \n",
    "def getEMD(P,Q, norm = 2):\n",
    "    \"\"\"\n",
    "    EMD computes the Earth Mover's Distance between\n",
    "    the distributions P and Q\n",
    "\n",
    "    P and Q are of shape (2,N)\n",
    "\n",
    "    Where the first row are the set of N features\n",
    "    The second row are the corresponding set of N weights\n",
    "\n",
    "    The norm defines the L-norm for the ground distance\n",
    "    Default is the Euclidean norm (norm = 2)\n",
    "    \"\"\"  \n",
    "\n",
    "    D = getDistMatrix(P[0], Q[0], norm)\n",
    "    F = getFlowMatrix(P, Q, D)\n",
    "\n",
    "    return EMD(F, D)\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def get_loss(pointclouds1, pointclouds2):\n",
    "    loss = tf.numpy_function(getEMD, [pointclouds1,pointclouds2], tf.float64)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_integers(n, shape):\n",
    "    sample = tf.random_uniform(shape, minval=0, maxval=tf.cast(n, 'float32'))\n",
    "    sample = tf.cast(sample, 'int32')\n",
    "    return sample\n",
    "\n",
    "def resample_rows_per_column(x):\n",
    "    \"\"\"Permute all rows for each column independently.\"\"\"\n",
    "    n_batch = tf.shape(x)[0]\n",
    "    n_dim = tf.shape(x)[1]\n",
    "    row_indices = sample_integers(n_batch, (n_batch * n_dim,))\n",
    "    col_indices = tf.tile(tf.range(n_dim), [n_batch])\n",
    "    indices = tf.transpose(tf.stack([row_indices, col_indices]))\n",
    "    x_perm = tf.gather_nd(x, indices)\n",
    "    x_perm = tf.reshape(x_perm, (n_batch, n_dim))\n",
    "    return x_perm\n",
    "\n",
    "def z_score(x):\n",
    "    \"\"\"\n",
    "    Z_scores each dimension of the data (across axis 0)\n",
    "    \"\"\"\n",
    "    #mean_vals = tf.reduce_mean(x,axis=0,keep_dims=True)\n",
    "    #std_vals = tf.sqrt(tf.reduce_var(x,axis=0,keep_dims=True))\n",
    "    mean_vals,var_vals = tf.nn.moments(x,axes=[0],keep_dims=True)\n",
    "    std_vals = tf.sqrt(var_vals)\n",
    "    x_normalized = (x - mean_vals)/std_vals\n",
    "    return x_normalized\n",
    "\n",
    "def cost_matrix(x,y,p=2):\n",
    "    \"Returns the cost matrix C_{ij}=|x_i - y_j|^p\"\n",
    "    x_col = tf.expand_dims(x,1)\n",
    "    y_lin = tf.expand_dims(y,0)\n",
    "    c = tf.reduce_sum((tf.abs(x_col-y_lin))**p,axis=2)\n",
    "    return c\n",
    "\n",
    "def asymmetric_loss(alpha):\n",
    "    def loss(y_true, y_pred):\n",
    "        delta = y_pred - y_true\n",
    "        return K.mean(K.square(delta) * \n",
    "                      K.square(K.sign(delta) + alpha), \n",
    "                      axis=-1)\n",
    "    return loss\n",
    "\n",
    "def outer_sinkhorn_loss(n,niter,epsilon, p=2):\n",
    "    def sinkhorn_loss(x,y):\n",
    "        \"\"\"\n",
    "        Given two emprical measures with n points each with locations x and y\n",
    "        outputs an approximation of the OT cost with regularization parameter epsilon\n",
    "        niter is the max. number of steps in sinkhorn loop\n",
    "\n",
    "        Inputs:\n",
    "            x,y:  The input sets representing the empirical measures.  Each are a tensor of shape (n,D)\n",
    "            epsilon:  The entropy weighting factor in the sinkhorn distance, epsilon -> 0 gets closer to the true wasserstein distance\n",
    "            n:  The number of support points in the empirical measures\n",
    "            niter:  The number of iterations in the sinkhorn algorithm, more iterations yields a more accurate estimate\n",
    "        Outputs:\n",
    "\n",
    "        \"\"\"\n",
    "        # The Sinkhorn algorithm takes as input three variables :\n",
    "        C = cost_matrix(x, y,p=p)  # Wasserstein cost function\n",
    "\n",
    "        # both marginals are fixed with equal weights\n",
    "        mu = tf.constant(1.0/n,shape=[n])\n",
    "        nu = tf.constant(1.0/n,shape=[n])\n",
    "        # Elementary operations\n",
    "        def M(u,v):\n",
    "            \"Modified cost for logarithmic updates\"\n",
    "            \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "            return (-C + tf.expand_dims(u,1) + tf.expand_dims(v,0) )/epsilon\n",
    "        def lse(A):\n",
    "            return tf.reduce_logsumexp(A,axis=1,keepdims=True)\n",
    "\n",
    "        # Actual Sinkhorn loop\n",
    "        u, v = 0. * mu, 0. * nu\n",
    "        for i in range(niter):\n",
    "            u = epsilon * (tf.math.log(mu) - tf.squeeze(lse(M(u, v)) )  ) + u\n",
    "            v = epsilon * (tf.math.log(nu) - tf.squeeze( lse(tf.transpose(M(u, v))) ) ) + v\n",
    "\n",
    "        u_final,v_final = u,v\n",
    "        pi = tf.exp(M(u_final,v_final))\n",
    "        cost = tf.reduce_sum(pi*C)\n",
    "        return cost\n",
    "    return sinkhorn_loss\n",
    "    \n",
    "def sinkhorn_from_product(x,epsilon,n,niter,z_score=False):\n",
    "    y = resample_rows_per_column(x)\n",
    "    if z_score:\n",
    "        x = z_score(x)\n",
    "        y = z_score(y)\n",
    "    return sinkhorn_loss(x,y,epsilon,n,niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wishbone\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.optimizers import adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, beta = 1.0):\n",
    "    r = tf.transpose(x1)\n",
    "    r = tf.expand_dims(r, 2)\n",
    "    return tf.reduce_sum(K.exp( -beta * K.square(r - x2)), axis=-1)\n",
    "def MMD(x1, x2, beta):\n",
    "    x1x1 = gaussian_kernel(x1, x1, beta)\n",
    "    x1x2 = gaussian_kernel(x1, x2, beta)\n",
    "    x2x2 = gaussian_kernel(x2, x2, beta)\n",
    "    diff = tf.reduce_mean(x1x1) - 2 * tf.reduce_mean(x1x2) + tf.reduce_mean(x2x2)\n",
    "    return diff\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def emd_loss(y_true, y_pred):\n",
    "    y_true = y_true[:cells_to_use, ]\n",
    "    return tf.py_function(wasserstein_distance, [y_true, y_pred], tf.float32)\n",
    "def compute_cosine_distances(a, b):\n",
    "    # x shape is n_a * dim\n",
    "    # y shape is n_b * dim\n",
    "    # results shape is n_a * n_b\n",
    "\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1)\n",
    "    distance = 1 - tf.matmul(normalize_a, normalize_b, transpose_b=True)\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def val_loss_mmd2(x1, x2):\n",
    "    x1 = tf.transpose(x1)\n",
    "    x2 = tf.transpose(x2)\n",
    "    res= MMD(x2, x1, 1)\n",
    "    return res\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    #P = np.transpose(P)\n",
    "    #Q = np.transpose(Q)\n",
    "    #print(P.shape)\n",
    "    #P, Q = Q, P\n",
    "    # Then compute their means\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "    \n",
    "    # Compute their covariance\n",
    "    cov_P = np.cov(P, rowvar=False)\n",
    "    cov_Q = np.cov(Q, rowvar=False)    \n",
    "        \n",
    "    cov_Q_inv = np.linalg.inv(cov_Q)\n",
    "    \n",
    "    # Compute KL divergence\n",
    "    KL_div = np.log(np.linalg.det(cov_Q)/np.linalg.det(cov_P)) - mu_P.shape[0] + np.trace(cov_Q_inv@cov_P) + \\\n",
    "                (mu_P - mu_Q).T@cov_Q_inv@(mu_P - mu_Q)\n",
    "    \n",
    "    KL_div = 0.5 * KL_div\n",
    "    \n",
    "    return KL_div/P.shape[0]\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def kl_in_tf(A, B):\n",
    "    A = A[:cells_to_use, ]\n",
    "    ans = tf.numpy_function(KL_div, [A, B], tf.float64)\n",
    "    return ans\n",
    "\n",
    "def mmd2(x1, x2):\n",
    "    tf.shape(x1)\n",
    "    x1 = tf.transpose(x1)\n",
    "    x2 = tf.transpose(x2)\n",
    "    res= MMD(x2, x1, 1)\n",
    "    \n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def mmd2(x1, x2):\n",
    "    x1 = tf.transpose(x1)\n",
    "    x2 = tf.transpose(x2)\n",
    "    res= MMD(x2, x1, 1)\n",
    "    return res\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def d_loss(x1, x2):\n",
    "    ans = tf.reduce_mean(x2, axis = 0) - tf.reduce_mean(x1, axis = 0)\n",
    "    ans = tf.reduce_mean(tf.square(ans))\n",
    "    return tf.abs(ans)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def mmd2_1(x1, x2):\n",
    "    x1 = x1[:cells_to_use, ]\n",
    "    x1 = tf.transpose(x1)\n",
    "    x2 = tf.transpose(x2)\n",
    "    res= MMD(x2, x1, 1)\n",
    "    return res\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def d_loss_1(x1, x2):\n",
    "    x1 = x1[:cells_to_use, ]\n",
    "    ans = tf.reduce_mean(x2, axis = 0) - tf.reduce_mean(x1, axis = 0)\n",
    "    ans = tf.reduce_mean(tf.square(ans))\n",
    "    return tf.abs(ans)\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def distance_matrix(x1, x2):\n",
    "    x1 = x1[cells_to_use:, ]\n",
    "    ac = tf.expand_dims(x1, axis =0)\n",
    "    c = tf.expand_dims(x1, axis =1)\n",
    "    res_x1 = tf.norm(ac-c, axis= 2)\n",
    "    #res_x1 = compute_cosine_distances(ac, c)\n",
    "    ac = tf.expand_dims(x2, axis =0)\n",
    "    c = tf.expand_dims(x2, axis =1)\n",
    "    res_x2 = tf.norm(ac-c, axis= 2)\n",
    "    #res_x2 = compute_cosine_distances(ac, c)\n",
    "    ans = tf.reduce_mean(tf.square(res_x2-res_x1))\n",
    "    return ans\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def wasserstein(x1, x2):\n",
    "    x1 = x1[:cells_to_use, ]\n",
    "    cdf_true = K.cumsum(x1, axis=0)\n",
    "    cdf_pred = K.cumsum(x2, axis=0)\n",
    "    emd = K.sqrt(K.mean(K.square(cdf_true - cdf_pred), axis=0))\n",
    "    return K.mean(emd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def mmd2_1(x1, x2):\n",
    "    x1 = x1[:cells_to_use, ]\n",
    "    x1 = tf.transpose(x1)\n",
    "    x2 = tf.transpose(x2)\n",
    "    #res= MMD(x2, x1, 1)\n",
    "    res = maximum_mean_discrepancy(x1, x2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_mean_discrepancy(x, y):\n",
    "    x_kernel = tf.reduce_mean(tf.exp(-tf.square(tf.math.subtract(x, tf.expand_dims(x, 1)))))\n",
    "    y_kernel = tf.reduce_mean(tf.exp(-tf.square(tf.math.subtract(y, tf.expand_dims(y, 1)))))\n",
    "    xy_kernel = tf.reduce_mean(tf.exp(-tf.square(tf.math.subtract(x, tf.expand_dims(y, 1)))))\n",
    "    return x_kernel + y_kernel - 2 * xy_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DEFINE MODELS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, y, size, steps_per_epoch):\n",
    "    for i in range(steps_per_epoch):\n",
    "        batch_x= x.sample(n = size, replace=True)\n",
    "        batch_y= y.sample(n = size, replace=True)\n",
    "        #yield (np.array(batch_x), np.concatenate((np.array(batch_y), np.array(batch_x)), axis =0))   ###why concat\n",
    "        yield (np.array(batch_x), np.array(batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def model_that_corrects_batch_effect(unchanged_df_to_be_used_for_correction_from_batch,\n",
    "                                     to_be_changed_batch_df,\n",
    "                                     depth_of_the_model,\n",
    "                                     epochs,\n",
    "                                     steps_per_epoch,\n",
    "                                     cells_per_batch,\n",
    "                                     val_unchanged_df_to_be_used_for_correction_from_batch,\n",
    "                                     val_to_be_changed_batch_df,\n",
    "                                     first_activation,\n",
    "                                     internal_activation\n",
    "                                    ):\n",
    "    l1_lambda = 0.01\n",
    "    l2_lambda = 0.01\n",
    "    \n",
    "    input_shape = to_be_changed_batch_df.shape[1]\n",
    "    inp = Input((input_shape,), name=\"input1\")\n",
    "    x = Dense(input_shape, activation=first_activation, kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(inp)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    n = input_shape\n",
    "    for i in range(depth_of_the_model):\n",
    "        n = n + 5\n",
    "        x = Dense(n, activation= internal_activation, kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(x)\n",
    "    output1 = Dense(input_shape, activation='relu', name=\"output11\", kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(x)    \n",
    "    adamoptimizer = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "    new_model = Model(inputs=inp, outputs=[output1, output1])\n",
    "    new_model.compile(\n",
    "                     loss = [mmd2_1,\n",
    "                              #wasserstein, \n",
    "                              d_loss_1, \n",
    "                              distance_matrix,\n",
    "                              #KLDivergence()\n",
    "                             ],\n",
    "                     loss_weights= [0.50,\n",
    "                                    0.30,\n",
    "                                    0.20,\n",
    "                                    #0.15, \n",
    "                                    #0.15\n",
    "                                   ],\n",
    "                \n",
    "                      #loss= outer_sinkhorn_loss(n = cells_per_batch,niter = epochs, epsilon = 0.001, p=2), \n",
    "                       #loss = get_loss,\n",
    "                      optimizer= adamoptimizer)\n",
    "    es = EarlyStopping(monitor= \"val_loss\", mode='min', verbose=2, patience=10)\n",
    "    if val_unchanged_df_to_be_used_for_correction_from_batch.empty:\n",
    "                \n",
    "        hist = new_model.fit_generator(generator(to_be_changed_batch_df, unchanged_df_to_be_used_for_correction_from_batch, \n",
    "                                       cells_per_batch, steps_per_epoch*epochs), \n",
    "                             steps_per_epoch = steps_per_epoch, \n",
    "                             epochs= epochs, \n",
    "                            )\n",
    "    elif not val_unchanged_df_to_be_used_for_correction_from_batch.empty:\n",
    "        hist = new_model.fit_generator(generator(to_be_changed_batch_df, unchanged_df_to_be_used_for_correction_from_batch, cells_per_batch, steps_per_epoch*epochs), \n",
    "                             validation_data = generator(val_to_be_changed_batch_df, val_unchanged_df_to_be_used_for_correction_from_batch, cells_per_batch, ((steps_per_epoch+5)*(epochs+5))),\n",
    "                             validation_steps = steps_per_epoch,\n",
    "                             steps_per_epoch = steps_per_epoch, \n",
    "                             epochs= epochs, \n",
    "                             \n",
    "                             callbacks=[es]\n",
    "                            )\n",
    "        \n",
    "    return([new_model, hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###generator\n",
    "def generator2(x, size, steps_per_epoch):\n",
    "    for i in range(steps_per_epoch):\n",
    "        batch_x= x.sample(n = size, replace=True)\n",
    "        yield np.array(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###model\n",
    "def model_2():\n",
    "    l1_lambda = 0.01\n",
    "    l2_lambda = 0.01\n",
    "    depth_of_the_model = 0\n",
    "    input_shape = 22  ## hardcoded\n",
    "    inp = Input((input_shape,), name=\"input1\")\n",
    "    x = Dense(input_shape, activation='linear', kernel_regularizer=regularizers.l1(l1_lambda))(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    n = input_shape\n",
    "    for i in range(depth_of_the_model):\n",
    "        n = n + 5\n",
    "        x = Dense(n, activation='elu', kernel_regularizer=regularizers.l1(l1_lambda))(x)\n",
    "    output1 = Dense(input_shape, activation='relu', name=\"output11\", kernel_regularizer=regularizers.l1(l1_lambda))(x)    \n",
    "    adamoptimizer = tf.keras.optimizers.Adam(learning_rate= 0.01)\n",
    "    new_model = Model(inputs=inp, outputs=output1 )\n",
    "    new_model.compile(\n",
    "        loss = [mmd2, d_loss,],\n",
    "        loss_weights= [0.50, 0.50,],\n",
    "        #loss = mmd2, \n",
    "        optimizer= adamoptimizer)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/okokok\"\n",
    "list_files = os.listdir(path)\n",
    "\n",
    "n1 = 6\n",
    "new1 = list_files[n1]\n",
    "print(new1)\n",
    "new1 = path+\"/\" + new1\n",
    "scdata1 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new1),\n",
    "            cofactor= None)\n",
    "\n",
    "n2= 8\n",
    "new2 = list_files[n2]\n",
    "print(new2)\n",
    "new2 = path+\"/\" + new2\n",
    "scdata2 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new2),\n",
    "            cofactor= None)\n",
    "\n",
    "new3 = list_files[n1+1]\n",
    "print(new3)\n",
    "new3 = path+\"/\" + new3\n",
    "scdata3 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new3),\n",
    "            cofactor= None)\n",
    "\n",
    "new4 = list_files[n2+1]\n",
    "print(new4)\n",
    "new4 = path+\"/\" + new4\n",
    "scdata4 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new4),\n",
    "            cofactor= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(scdata1.data)\n",
    "\n",
    "scaler5 = StandardScaler()\n",
    "scaler5.fit(scdata3.data)\n",
    "\n",
    "column_names = scdata1.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata1.normalize()\n",
    "scdata2.normalize()\n",
    "scdata3.normalize()\n",
    "scdata4.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###data\n",
    "source_dataset = scdata1.data\n",
    "source_dataset1 = scdata2.data\n",
    "target_dataset = scdata3.data\n",
    "target_dataset2 = scdata4.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = scdata1.data\n",
    "B = scdata2.data\n",
    "C = scdata3.data\n",
    "D = scdata4.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the base mmd value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_value_for_anchor = MMD(A, B, 1)\n",
    "base_value_for_anchor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_value_for_validation = MMD(C, D, 1)\n",
    "base_value_for_validation.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "number_of_cell =2000\n",
    "source_iterator = generator2(source_dataset, number_of_cell, num_epochs)\n",
    "source_iterator1 = generator2(source_dataset1, number_of_cell, num_epochs)\n",
    "target_iterator = generator2(target_dataset, number_of_cell, num_epochs)\n",
    "target_iterator1 = generator2(target_dataset, number_of_cell, num_epochs)\n",
    "# Iterate over epochs\n",
    "new_model = model_2()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "    source_batch = next(source_iterator)\n",
    "    source_batch1 = next(source_iterator1)\n",
    "    loss = new_model.train_on_batch(source_batch, source_batch1)  # Update the model with the source dataset\n",
    "    print(f'Custom Loss: {loss}')\n",
    "    \n",
    "    target_batch = next(target_iterator)\n",
    "    target_batch1 = next(target_iterator1)\n",
    "    loss = new_model.train_on_batch(target_batch, target_batch1)  # Update the model with the target dataset\n",
    "    print(f'Custom Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cells_in_each_iteration = 500\n",
    "first = f\"Batch{N1}\"    ###blue\n",
    "second = f\"Batch{N2}\"   ###red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "B= scdata2.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "B = B.reset_index(drop=True)\n",
    "AB_merged = A.append(B, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(new_model.predict(B))\n",
    "res.columns = B.columns\n",
    "\n",
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "\n",
    "AB_merged = A.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = C.reset_index(drop=True)\n",
    "D= scdata4.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "D = D.reset_index(drop=True)\n",
    "CD_merged = C.append(D, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(new_model.predict(D))\n",
    "res.columns = D.columns\n",
    "\n",
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = A.reset_index(drop=True)\n",
    "\n",
    "CD_merged = C.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "###\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_to_use = 1500\n",
    "model = model_that_corrects_batch_effect(A, B, 1, 10,5, cells_to_use, C, D,\n",
    "                                         \"linear\",\n",
    "                                         \"linear\"\n",
    "#                                          pd.DataFrame(),\n",
    "#                                          pd.DataFrame(),\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist= model[1]\n",
    "model = model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_one = hist.history[\"loss\"]\n",
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"], c = \"b\", label = \"training\")\n",
    "plt.plot([x for x in hist.history[\"val_loss\"]], c = \"r\", label =\"validation\")\n",
    "#plt.plot(first_one, c = \"r\", label =\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f\"Training and validation Losses \\nof model for correcting batch effect \\n between batches {N1} and {N2}\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Plot pca before correction of two batches</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cells_in_each_iteration = 500\n",
    "first = f\"Batch{N1}\"    ###blue\n",
    "second = f\"Batch{N2}\"   ###red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "B= scdata2.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "B = B.reset_index(drop=True)\n",
    "AB_merged = A.append(B, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.predict(B)[0])\n",
    "res.columns = B.columns\n",
    "\n",
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "\n",
    "AB_merged = A.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = C.reset_index(drop=True)\n",
    "D= scdata4.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "D = D.reset_index(drop=True)\n",
    "CD_merged = C.append(D, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.predict(D)[0])\n",
    "res.columns = D.columns\n",
    "\n",
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = A.reset_index(drop=True)\n",
    "\n",
    "CD_merged = C.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = scdata1.data\n",
    "B = scdata2.data\n",
    "C = scdata3.data\n",
    "D = scdata4.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.predict(D)[0])\n",
    "res.columns = D.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"MLcorrectedbatch2trainingdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sampled = res.sample(n = 500, replace=True)\n",
    "fig = plt.figure(figsize = (15,20))\n",
    "ax = fig.gca()\n",
    "fig = res_sampled.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_sampled = C.sample(n = 500, replace=True)\n",
    "fig = plt.figure(figsize = (15,20))\n",
    "ax = fig.gca()\n",
    "fig = C_sampled.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = tf.convert_to_tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(model.predict(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd2(res, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = tf.convert_to_tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(res2, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean(K.square(res.max() - A.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = mmd2(A, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = B.iloc[1:5]\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = correct_df_with_a_model(j, model)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(B.iloc[3].values.reshape(1, 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_merged.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [0, 0, 2], [1,2,3], [1, 2, 2]])\n",
    "b = np.array([[1,2,3], [1, 2, 2], [0, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tf.convert_to_tensor(a)\n",
    "b= tf.convert_to_tensor(b)\n",
    "a = tf.cast(a, 'float64')\n",
    "b = tf.cast(b, 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(a, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(tf.transpose(a), axis =0)\n",
    "tf.expand_dims(tf.transpose(a), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(tf.transpose(a), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second  = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (first > second):\n",
    "    first, second = second, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.reshape(a, [1, first, second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = tf.reshape(a, [first,1,second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.norm(ac-c, axis= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.square(z-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean(tf.norm(a-b, axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowwise_diff(a, b):\n",
    "    length = a.shape[0]\n",
    "    result = sum([np.linalg.norm(a[i] - b[i]) for i in range(length) ])/length\n",
    "    result = tf.convert_to_tensor(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(rowwise_diff(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean([np.linalg.norm(a[i] - b[i]) for i in range(a.shape[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
